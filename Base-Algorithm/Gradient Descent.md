# 梯度下降法（Gradient Descent）

梯度下降法可以和梯度上升法相互转化

梯度下降不一定能够找到全局的最优解，有可能是一个局部最优解。如果损失函数是凸函数时，梯度下降法得到的解就一定是全局最优解。

## 算法调优

1. 步长选择。取决于数据样本，可以多取一些值，从大到小，分别运行算法，比较迭代效果。若损失函数在变小，说明取值有效，否则要增大步长。步长太大，会导致迭代过快，可能错过最优解。步长太小，迭代速度太慢，算法耗时太久。所以，算法步长需要多次尝试后才能得到一个较优的值。
2. 初始值选择。初始值不同，损失函数最小值可能不同。建议多次选择不同初始值运行算法，选择损失函数最小的初始值作为算法初始值，减小局部最优解风险。
3. 归一化。样本的不同特征取值范围不一样，可能会导致迭代很慢。进行归一化，可以减少特征取值范围影响。一般做如下转化：
$$
    \frac{x-\overline{x}}{std(x)}
$$

## 子算法

### 批量梯度下降法（Batch Gradient Descent，BGD）

梯度下降法最常用的形式，在更新参数时使用所有的样本来进行更新

### 随机梯度下降法（Stochastic Gradient Descent，SGD）

和批量梯度下降法原理类似，区别在与求梯度时没有用所有的m个样本的数据，而是仅仅选取一个样本j来求梯度。

1. 对于训练速度来说，随机梯度下降法由于每次仅仅采用一个样本来迭代，训练速度很快，而批量梯度下降法在样本量很大的时候，训练速度不能让人满意。
2. 对于准确度来说，随机梯度下降法用于仅仅用一个样本决定梯度方向，导致解很有可能不是最优。
3. 对于收敛速度来说，由于随机梯度下降法一次迭代一个样本，导致迭代方向变化很大，不能很快的收敛到局部最优解。

### 小批量梯度下降法（Mini-batch Gradient Descent，MGD）

是批量梯度下降法和随机梯度下降法的折衷，也就是对于m个样本，我们采用x个样子来迭代，1<x<m。一般可以取x=10，当然根据样本的数据，可以调整这个x的值。

## 梯度下降法优缺点

1. 梯度下降法和最小二乘法相比，梯度下降法需要选择步长，而最小二乘法不需要。
2. 梯度下降法是迭代求解，最小二乘法是计算解析解。如果样本量不算很大，且存在解析解，最小二乘法比起梯度下降法要有优势，计算速度很快。但是如果样本量很大，用最小二乘法由于需要求一个超级大的逆矩阵，这时就很难或者很慢才能求解解析解了，使用迭代的梯度下降法比较有优势。

1. 梯度下降法和牛顿法/拟牛顿法相比，两者都是迭代求解，不过梯度下降法是梯度求解，而牛顿法/拟牛顿法是用二阶的海森矩阵的逆矩阵或伪逆矩阵求解。相对而言，使用牛顿法/拟牛顿法收敛更快。但是每次迭代的时间比梯度下降法长。
