{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scikit-Learn决策树"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scikit-learn决策树算法是基于CART树算法调优过的。\n",
    "\n",
    "- DecisionTreeClassifier：分类决策树\n",
    "- DecisionTreeRegressor：回归决策树"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 调参\n",
    "\n",
    "### criterion\n",
    "\n",
    "特征选择标准\n",
    "\n",
    "- DecisionTreeClassifier：'gini'：基尼系数,'entropy'：增益\n",
    "- DecisionTreeRegressor：'mse'：均方差，'mae'：和均值之差的绝对值之和，'mse'更加精确\n",
    "\n",
    "\n",
    "### splitter\n",
    "\n",
    "特征划分点选择标准\n",
    "\n",
    "- 'best':在特征的所有划分点中找出最优的划分点\n",
    "- 'random':随机的在部分划分点中找局部最优的划分点\n",
    "\n",
    "默认的\"best\"适合样本量不大的时候，而如果样本数据量非常大，此时决策树构建推荐\"random\" \n",
    "\n",
    "\n",
    "### max_features\n",
    "\n",
    "划分时考虑的最大特征数，默认为\"None\"\n",
    "\n",
    "- \"None\": 划分时考虑所有的特征数\n",
    "- \"log2\": 划分时最多考虑$ \\log_2 N $个特征\n",
    "- \"sqrt\" 或 \"auto\": 划分时最多考虑$ \\sqrt N $\n",
    "- 整数: 划分时考虑的特征绝对数\n",
    "- 浮点数: 划分时考虑的特征百分比\n",
    "\n",
    "若样本特征数不多，则直接用默认值即可；若样本特征数非常多，可考虑其它值。\n",
    "\n",
    "\n",
    "### max_depth\n",
    "\n",
    "决策树最大深度，默认不输入，表示不限制子树的深度。\n",
    "\n",
    "特征较少或样本少时，可不输入；若样本量多、特征数量多时，建议限制最大深度，具体取值取决于数据的分布。常用取值10-100。\n",
    "\n",
    "\n",
    "### min_samples_split\n",
    "\n",
    "内部节点再划分所需最小样本数，默认为2。\n",
    "\n",
    "如果样本较少，可以不输入。如果样本量大，建议增大该值。\n",
    "\n",
    "\n",
    "### min_samples_leaf\n",
    "\n",
    "叶子节点最少样本数，如果某叶子节点样本数目小于最小样本数，则该叶子节点会和兄弟节点一起被剪枝。默认为1。\n",
    "\n",
    "- 整数: 最少样本数值\n",
    "- 浮点数: 最少样本数占总样本数的百分比\n",
    "\n",
    "如果样本较少，可以不输入。如果样本量大，建议增大该值。\n",
    "\n",
    "\n",
    "### min_weight_fraction_leaf\n",
    "\n",
    "叶子节点最小的样本权重和，如果小于这个值，则该叶子节点会和兄弟节点一起被剪枝。默认为0，表示不考虑权重问题。\n",
    "\n",
    "如果样本缺失值较多，或样本类别偏差很大，需要引入样本权重。\n",
    "\n",
    "\n",
    "### max_leaf_nodes\n",
    "\n",
    "最大叶子节点数，用于防止过拟合，默认为\"None\"，表示不限制。\n",
    "\n",
    "如果特征不多，可不输入；如果特征多，可进行限制，通过交叉验证可以得到具体值。\n",
    "\n",
    "\n",
    "### class_weight\n",
    "\n",
    "类别权重，防止训练集某些类别的样本过多，导致训练的决策树过于偏向这些类别。默认为\"None\"，表示对类别分布没有偏倚。\n",
    "\n",
    "- \"balanced\": 算法自己计算权重，样本量少的类别对应权重高\n",
    "- 指定权重: 手动指定各个样本权重\n",
    "\n",
    "不适于回归树\n",
    "\n",
    "\n",
    "### min_impurity_split\n",
    "\n",
    "节点划分最小不纯度，限制了决策树的增长，如果某节点的不纯度(基尼系数，信息增益，均方差，绝对差)小于这个阈值，则该节点不再生成子节点。即为叶子节点。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 调参注意事项\n",
    "\n",
    "1. 当样本数量少、特征非常多的时候，决策树很容易过拟合。推荐先做维度规约，如PCA、特征选择（Losso）、独立成分分析（ICA），降低维度。\n",
    "\n",
    "2. 推荐决策树可视化，可以先限制决策树深度（比如最多3层），观察下生成决策树里数据的初步拟合情况，然后再决定是否增加深度。\n",
    "\n",
    "3. 训练模型前，先观察样本的类别情况，如果不均匀，就设置样本权重。\n",
    "\n",
    "4. 如果输入样本矩阵稀疏，推荐在拟合前调用csc_matrix稀疏化，在预测前调用csc_matrix稀疏化。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 决策树可视化"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
