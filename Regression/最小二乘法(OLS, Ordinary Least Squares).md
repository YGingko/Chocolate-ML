
# 最小二乘法（Ordinary Least Squares，OLS）
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>

又称最小平方法，是一种数学优化技术。通过最小化误差的平方和寻找数据的最佳函数匹配。形式如下：

$$ 目标函数 = \sum ({预测值-真实值})^2 $$

数学形式如下：

$$
    J(\theta) = ({X\theta - y})^2 
$$

求解方法：
1. 代数法求解，目标函数对各特征变量求偏导数，并令偏导数为0，得到方程组，求解得到$ \theta $
2. 矩阵求解，$ \theta = ({X^T}X)^{-1}{X^T}Y $

## 局限性

1. 若$ {X^T}X $不存在逆矩阵，则没有办法直接用最小二乘法，但梯度下降法仍然适用。建议对样本数据进行整理，去掉冗余特征。让$ {X^T}X $的行列式不为0，继续使用二乘法。
2. 当样本特征n非常大时，$ {X^T}X $逆矩阵计算量非常大，甚至不可行。建议特征数量超过10000时，使用梯度下降迭代法。或通过主成分分析降低特征的维度后再使用最小二乘法。
3. 若拟合函数不是线性的，则无法使用最小二乘法，但梯度下降法仍然适用。建议通过一些技巧转化为线性之后再使用。
4. 若样本量非常少，少于特征数量时，拟合方程是欠定方程，常用的优化方法都无法去拟合数据；当样本数量和特征数量相等时，方程组直接求解即可；当样本数量大于特征数量时，拟合方程是超定方程，此即为最小二乘法应用场景。
